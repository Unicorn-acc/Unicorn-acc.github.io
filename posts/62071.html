<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>目标检测-04-①YOLO v1~v3、②YOLO v3 SPP的理论讲解 | Miraclo’s Blog</title><meta name="author" content="Miraclo"><meta name="copyright" content="Miraclo"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="参考内容来自霹雳吧啦Wz up主的b站链接：https:&#x2F;&#x2F;space.bilibili.com&#x2F;18161609&#x2F;channel&#x2F;index up主的github：https:&#x2F;&#x2F;github.com&#x2F;WZMIAOMIAO&#x2F;deep-learning-for-image-processing up主的CSDN博客：https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_37541097&#x2F;articl"><meta property="og:type" content="article"><meta property="og:title" content="目标检测-04-①YOLO v1~v3、②YOLO v3 SPP的理论讲解"><meta property="og:url" content="http://unicorn-acc.github.io/posts/62071.html"><meta property="og:site_name" content="Miraclo’s Blog"><meta property="og:description" content="参考内容来自霹雳吧啦Wz up主的b站链接：https:&#x2F;&#x2F;space.bilibili.com&#x2F;18161609&#x2F;channel&#x2F;index up主的github：https:&#x2F;&#x2F;github.com&#x2F;WZMIAOMIAO&#x2F;deep-learning-for-image-processing up主的CSDN博客：https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_37541097&#x2F;articl"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://w.wallhaven.cc/full/1p/wallhaven-1pjml1.jpg"><meta property="article:published_time" content="2022-11-18T10:20:03.000Z"><meta property="article:modified_time" content="2022-11-19T09:02:04.463Z"><meta property="article:author" content="Miraclo"><meta property="article:tag" content="DeepLearning"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://w.wallhaven.cc/full/1p/wallhaven-1pjml1.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://unicorn-acc.github.io/posts/62071"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"prismjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:2e3},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{limitCount:500,languages:{author:"作者: Miraclo",link:"链接: ",source:"来源: Miraclo’s Blog",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},lightbox:"fancybox",Snackbar:void 0,source:{justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js",css:"https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css"}},isPhotoFigcaption:!1,islazyload:!1,isAnchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"目标检测-04-①YOLO v1~v3、②YOLO v3 SPP的理论讲解",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2022-11-19 09:02:04"}</script><noscript><style type="text/css">#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,a){0!==a&&(a=864e5*a,t={value:t,expiry:(new Date).getTime()+a},localStorage.setItem(e,JSON.stringify(t)))},get:function(e){var t=localStorage.getItem(e);if(t){t=JSON.parse(t);if(!((new Date).getTime()>t.expiry))return t.value;localStorage.removeItem(e)}}},e.getScript=o=>new Promise((t,e)=>{const a=document.createElement("script");a.src=o,a.async=!0,a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};e=saveToLocal.get("theme"),"dark"===e?activateDarkMode():"light"===e&&activateLightMode(),e=saveToLocal.get("aside-status");void 0!==e&&("hide"===e?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="/css/mouth.css"><meta name="generator" content="Hexo 6.0.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./img/avatar.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">125</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">29</div></a></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-film"></i><span> 其他</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/algor-record/"><i class="fa-fw fas fa-gamepad"></i><span> 算法刷题记录</span></a></li><li><a class="site-page child" href="/DL-record/"><i class="fa-fw fas fa-trophy"></i><span> 深度学习记录</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:randomPost();"><i class="fa-fw fa-solid fa-shuffle"></i><span></span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://w.wallhaven.cc/full/1p/wallhaven-1pjml1.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Miraclo’s Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-film"></i><span> 其他</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/algor-record/"><i class="fa-fw fas fa-gamepad"></i><span> 算法刷题记录</span></a></li><li><a class="site-page child" href="/DL-record/"><i class="fa-fw fas fa-trophy"></i><span> 深度学习记录</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:randomPost();"><i class="fa-fw fa-solid fa-shuffle"></i><span></span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">目标检测-04-①YOLO v1~v3、②YOLO v3 SPP的理论讲解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2022-11-18T10:20:03.000Z" title="发表于 2022-11-18 10:20:03">2022-11-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">深度学习笔记</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/">网络模型</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/2%E3%80%81%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AF%87/">2、目标检测篇</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>35分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="目标检测-04-①YOLO v1~v3、②YOLO v3 SPP的理论讲解"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>参考内容来自霹雳吧啦Wz</p><p>up主的b站链接：https://space.bilibili.com/18161609/channel/index</p><p>up主的github：https://github.com/WZMIAOMIAO/deep-learning-for-image-processing</p><p>up主的CSDN博客：https://blog.csdn.net/qq_37541097/article/details/103482003</p><p>参考博文：https://blog.csdn.net/weixin_44878336/article/details/124759307</p></blockquote><h1 id="引言">0. 引言</h1><h2 id="安排">0.1 安排</h2><ul><li>YOLO v1 （简单理论）</li><li>YOLO v2 （简单理论）</li><li>YOLO v3 （详细理论）</li><li>YOLO v3 SPP （trick扩充 + 代码讲解）<font color="#FF0000"><strong>这个以后需要再看</strong></font></li><li>IoU、GIoU、DIoU、CIoU</li><li>Focal Loss</li></ul><h2 id="学习代码的步骤">0.2 学习代码的步骤</h2><p>想学习一个新的模型</p><ol type="1"><li>搜该网络的讲解 —— 大概有一个印象</li><li><strong>读原文（非常重要）</strong> —— 很多细节都是通过原论文实现的 —— 发现更多原来没有发现的知识点</li><li>读代码 —— github官方代码/复现的源码（⭐️多的）<ol type="1"><li>根据作者的<code>README.md</code>将代码跑通 —— <strong>跑通只是第一步而不是最后一步</strong>😂</li><li>分析网络搭建的部分 —— 结合原论文，还是比较好理解的</li><li>分析数据预处理和损失计算这<strong>两大</strong>部分 —— 不要小看这两大部分，比网络搭建有难度</li><li>网络搭建、数据预处理和损失计算看完之后，基本上就掌握了这个网络的核心技术点</li><li>看代码的过程中是需要结合原论文进行参考的。在读原论文的时候，有些地方是不太好理解的，而结合代码就可以进一步理解之前很难理解的部分</li></ol></li></ol><h1 id="yolo-v1">1. YOLO v1</h1><p>论文题目：You Only Look Once: Unified, Real-Time Object Detection 论文地址：https://arxiv.org/abs/1506.02640</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181029462.png"> YOLO是目标检测One-stage的经典网络。YOLO v1：</p><ul><li>2016年CVPR发表的论文</li><li>PASCAL 2007的mAP为63.4%（SSD为74.3%，Faster R-CNN为73.2%）</li><li>输入图片大小为448×448，推理速度为45 FPS （SSD为300×300，59FPS）</li></ul><blockquote><p>YOLO v1在mAP上比不过同年的SSD和之前的Faster R-CNN 在速度上秒杀Faster R-CNN，但打不过同年的SSD 😂 YOLO v1相比同年发布的SSD是没有什么优势的</p></blockquote><h2 id="核心思想">1.1 核心思想</h2><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181050597.png"></p><h2 id="第一步划分grid-cell">1.2 第一步：划分grid cell</h2><p>将输入图片分为 7×7个网格(grid cell)，如果<strong>某个目标GTBox的中心</strong>落在这个网格当中，则这个网络就负责预测这个目标。</p><h2 id="第二步每个grid-cell需要做的事情">1.3 第二步：每个grid cell需要做的事情</h2><p>每个网络(grid cell)要预测2个BBox (Bounding Box)，每个BBox要<strong>预测目标的位置信息</strong> <span class="math inline">\((x , y , w , h )\)</span> 之外还要<strong>预测一个confidence值</strong>（目标分类值，和类别没关系，是IoU×一个系数，<strong>是一个数</strong>）。每个网络还需要<strong>预测C个类别分数</strong>（<strong>是一个向量</strong>[num_classes, ]）。</p><blockquote><p><span class="math inline">\((x,y)\)</span>：相对grid cell的目标边界框 —— 相对值 (grid cell) <span class="math inline">\(\in [0, 1]\)</span></p><p>$ (w, h)$：相对整张图片的宽度和高度 —— 相对值 (image) $ $</p><p>confidence：预测框与GTBox的IoU值，再乘$ $。</p><p>其中， <img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181036004.png" style="zoom:80%"></p><p>Note:</p><ul><li><strong>YOLO v1中是直接预测BBox的坐标</strong>，而不像Faster R-CNN和SSD那样，是预测Anchor的回归参数的！</li><li>在YOLO v1中是没有Anchor这个概念（直接预测BBox的坐标，而不是预测回归参数）</li></ul></blockquote><blockquote><p>在<code>eval()</code>时，对于<strong>每个目标的概率</strong>是通过<strong>条件类别概率</strong>乘上confidence参数的</p><p>对于每一个网格而言，它有 C 类别，那么就会有 C 个类别分数，即<strong>条件类别概率: <span class="math inline">\(Pr(Class_i | Object)\)</span>—— 它是目标前提下的概率</strong>。而confidence就是<span class="math inline">\(Pr(Object) * IoU ^{\text{truth}}_{\text{pred}}\)</span>。<span class="math inline">\(Pr(Classi _ii | Object) * Pr(Object)\)</span>就可以得到<span class="math inline">\(Pr(Class_i)\)</span>，公式如下：</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181039208.png" style="zoom:80%"></p><p>通过这个表达式我们可以知道：</p><ul><li>它为某个目标的概率</li><li>预测的BBox和GTBBox的重合程度（IoU）</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181043266.png" style="zoom:67%"></p><p>一个网格需生成两个BBox，每个BBox除了要预测4个坐标外，还需要预测一个confidence（置信度），</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181056221.png" style="zoom:80%"></p><p>以VOC2012 20个类别举例，每个网格预测2个BBox和对应的confidence，还有20个类别得分==&gt; 共30个channel</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181043425.png" style="zoom:50%"></p><h2 id="网络结构">1.4 网络结构</h2><figure><img src="https://img-blog.csdnimg.cn/2ab62250d3864381803e990d47c4f25c.png" alt="在这里插入图片描述"><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><blockquote><p>如果没有标<code>s</code>，那么它默认的stride=1</p></blockquote><p>Q1： <code>fc(1470)</code>中的1470是怎么来的？ A1：因为输出是[7, 7, 30]，而这个输出特征图是经过reshape得到的，所以经过FC之后，输出的一维向量shape应该为1470。</p><p>Q2：为什么是[7, 7, 30]？ A2：7×7代表将图片划分为7×7个网格，30为每个网格的信息（参数个数）。</p><h2 id="损失函数">1.5 损失函数</h2><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181100965.png" style="zoom:50%"></p><p>在计算这些损失时，主要使用的是<strong>误差平方和</strong>这个函数进行的</p><p>误差平方和： <span class="math display">\[ 误差=(预测的x−真实的x)^2+(预测的y−真实的y)^2=(误差1)^2+(误差2)^2 \]</span> 这就是误差平方和😂</p><h3 id="bbox损失">1.5.1 BBox损失</h3><p>​ 1.5.1 BBox损失</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181105392.png" style="zoom:67%"></p><p>其中：</p><ul><li><span class="math inline">\(x_i, y_i, w_i, h_i\)</span>为预测值</li><li><span class="math inline">\(\hat{x_i}, \hat{y_i}, \hat{w_i}, \hat{h_i}\)</span>为GT值</li><li>$ ^{}_{ij}$表示当前为目标时为1，不为目标时为0 —— 正样本为1，负样本为0</li></ul><p>对于<span class="math inline">\(w\)</span>和<span class="math inline">\(h\)</span>的损失，加的目的是<font color="#FF0000">放大[小目标]的损失</font>（不同大小的目标，偏移相同的距离得到损失是不一样的）</p><figure><img src="https://img-blog.csdnimg.cn/517a4c05eedb4ef78acdc43b750230df.png#pic_center" alt="在这里插入图片描述"><figcaption aria-hidden="true">在这里插入图片描述</figcaption></figure><p>如果使用<span class="math inline">\(y=x\)</span>这样的策略，那么在相同偏移量的情况下，二者的loss是相同的。看右面的图，很明显，对于小目标而言它位移相对更大，理应有更大的损失。</p><p>为了达到这样的效果，作者使用了开根号，对于<span class="math inline">\(y=\sqrt{x}\)</span>来说，<span class="math inline">\(x\)</span>越大，<span class="math inline">\(y\)</span>越平缓。</p><h3 id="confidence损失">1.5.2 confidence损失</h3><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181107432.png" style="zoom:67%"></p><p>其中：</p><ul><li><span class="math inline">\(\mathbb{1}^{\text{obj}}_{ij}\)</span>表示当前为目标时为1，不为目标时为0 —— 正样本为1，负样本为0</li><li><span class="math inline">\(\mathbb{1}^{\text{noobj}}_{ij}\)</span>表示当前不为目标时为1，是目标时为0 —— 负样本为1，正样本为0</li><li><span class="math inline">\(\lambda_{\text{noobj}}\)</span>表示不为目标时的一个系数 —— <strong>负样本置信度误差的权重</strong></li></ul><p>​ 因为之前就说过了，<strong>这里的confidence置信度指的是当前预测的BBox和GTBox的IoU值，即该预测BBox为张样本的概率</strong>。而不是当前BBox的类别概率。</p><blockquote><p><span class="math inline">\(Confidence = Pr(Object) * IoU^{\text{truth}}_{\text{pred}}\)</span></p></blockquote><p>对于正样本，它的真实值<span class="math inline">\(\hat{C_i}\)</span>为1；对于负样本，它的真实值$ $应该为0。</p><h3 id="类别损失">1.5.3 类别损失</h3><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181110966.png" style="zoom:80%"> 其中：</p><ul><li><span class="math inline">\(p_i(c)\)</span>为该grid cell的类别预测值为<span class="math inline">\(c\)</span>类别的概率</li><li><span class="math inline">\(\hat{p_i}(c)\)</span>为该grid cell对应的GTBox的<span class="math inline">\(c\)</span>类别的概率（二值化，只能是0或1）</li></ul><h2 id="yolo-v1存在的问题">1.6 YOLO v1存在的问题</h2><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181111789.png" style="zoom:67%"></p><ol type="1"><li>对密集目标检测效果很差 —— 比如有一群鸟 —— 原因是grid cell数量少且每个grid cell只预测两个BBox —— 最多预测7×7×2=98个目标</li><li>当目标出现新的尺寸或配置时，检测效果也很差</li><li>主要错误的原因是定位错误 —— 主要因为v1是直接预测目标坐标而不像Faster R-CNN和SSD那样预测回归参数 —— 基于此，从YOLO v2开始，使用基于Anchor的回归预测机制。</li></ol><h2 id="yolo-v1中的bounding-box-和-anchor-有什么区别">1.7 YOLO v1中的Bounding Box 和 Anchor 有什么区别？</h2><p>​ Bounding Box（BBox）译为边界框，在YOLO v1中它的中心位于所属的grid cell中心，它进行定位回归时，不是用回归参数一点一点去进行调整，而是<strong>直接预测目标的图像坐标</strong>，这就导致YOLO v1 BBox的定位回归很差 —— 野蛮生长。</p><p>​ Anchor译为锚框，像Faster R-CNN和SSD那样，先生成Anchor，然后<strong>在定位回归时预测的是回归参数，即anchor的移动参数</strong>。不是直接让anchor预测目标的位置，所以定位效果比YOLO v1要好 —— 微调。</p><h1 id="yolo-v2">2. YOLO v2</h1><p>论文题目：YOLO9000: Better, Faster, Stronger</p><p>论文地址：https://arxiv.org/abs/1612.08242</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181113371.png" style="zoom:67%"></p><h2 id="yolo-v2-与其他网络的性能对比">2.1 YOLO v2 与其他网络的性能对比</h2><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181130195.png"></p><h2 id="yolo-v2相比v1的尝试">2.2 YOLO v2相比v1的尝试</h2><ul><li>Batch Normalization</li><li>High Resolution Classifier</li><li>Convolutional With Anchor Boxes</li><li>Dimension Clusters</li><li>Direct Location Prediction</li><li>Fine-Grained Features</li><li>Multi-scale Training</li></ul><h3 id="batch-normalization">2.2.1 Batch Normalization</h3><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181130323.png" style="zoom:80%"></p><p>在YOLO v1时没有使用BN层，在v2中每个卷积层后面加上了BN层。好处：</p><ol type="1"><li>加速模型收敛</li><li>减少所需使用使用的正则化操作 —— BN对模型起到了正则化作用</li><li>提升2%的mAP</li><li>作者提出一个观点：使用了BN层就可以不用Dropout层</li></ol><h3 id="high-resolution-classifier高分辨率分类器">2.2.2 High Resolution Classifier，高分辨率分类器</h3><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181132721.png" style="zoom:67%"> 在YOLO v1中训练backbone的输入像素为224×224（主要是那个时候训练backbone基本上都是用224×224的图片），然而在YOLO v2中作者使用了一个更大输入尺寸 —— 448×448。即网络先用224×224的图片进行训练，训练完毕后再对其进行10个Epoch的微调（迁移学习），此时输入图片大小为448×448。</p><p>采用更大输入图片的分类器给目标检测网络带来哪些好处呢？4%mAP</p><h3 id="convolutional-with-anchor-boxes基于anchor-boxes的卷积">2.2.3 Convolutional With Anchor Boxes，基于Anchor Boxes的卷积</h3><p>Convolutional With Anchor Boxes，基于Anchor Boxes的卷积，说白了就是基于Anchor的目标边界框预测（不像v1中那样野蛮生长了）。</p><p>​ YOLO predicts the coordinates of bounding boxes directly using fully connected layers on top of the convolutional feature extractor.Instead of predicting coordinates directly Faster R-CNN predicts bounding boxes using hand-picked priors. Using only convolutional layers the region proposal network (RPN) in Faster R-CNN predicts offsets and confidences for anchor boxes. Since the prediction layer is convolutional, the RPN predicts these offsets at every location in a feature map. Predicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learn.</p><p>​ YOLO v1直接使用卷积特征提取器顶部的全连接层直接预测边界框的坐标。<strong>Faster R-CNN 使用手工挑选的先验预测边界框</strong>，而不是直接预测坐标。 Faster R-CNN 中的区域提议网络 (RPN) 仅使用卷积层来预测锚框的偏移量和置信度。 由于预测层是卷积层，因此 RPN 会在特征图中的每个位置预测这些偏移量。 <strong>预测偏移量而不是坐标可以简化问题并使网络更容易学习</strong>。</p><blockquote><p><strong>Faster R-CNN 使用手工挑选的先验预测边界框</strong>：Faster R-CNN先挑选了集中尺寸的预测框，然后对于每个预测特征图上的每个像素都会进行anchor的生成，即每个像素上都会生成这些先验的预测框，之后通过RPN网络预测其回归的参数和置信度（这里的置信度是该Anchor的类别概率，和YOLO v1中的confidence不同）。 这里作者对Faster R-CNN中坐标回归表示肯定</p></blockquote><p>We remove the fully connected layers from YOLO and use anchor boxes to predict bounding boxes. First we eliminate one pooling layer to make the output of the network’s convolutional layers higher resolution. We also shrink the network to operate on 416 input images instead of 448×448. We do this because we want an odd number of locations in our feature map so there is a single center cell. Objects, especially large objects, tend to occupy the center of the image so it’s good to have a single location right at the center to predict these objects instead of four locations that are all nearby. YOLO’s convolutional layers downsample the image by a factor of 32 so by using an input image of 416 we get an output feature map of 13 × 13.</p><p>我们从 YOLO 中移除全连接层，并使用锚框来预测边界框。 首先，我们消除了一个池化层，以使网络卷积层的输出具有更高的分辨率。 我们还缩小了网络以对 416 个输入图像而不是 448×448 进行操作。 我们这样做是因为我们想要在我们的特征图中有奇数个位置，所以只有一个中心单元。 目标，尤其是大目标，往往占据图像的中心，因此最好在中心有一个位置来预测这些物体，而不是四个都在附近的位置。 YOLO 的卷积层将图像下采样 32 倍，因此通过使用 416 的输入图像，我们得到 13 × 13 的输出特征图。</p><blockquote><p>这里说明了YOLO的architecture改动</p></blockquote><p>When we move to anchor boxes we also decouple the class prediction mechanism from the spatial location and instead predict class and objectness for every anchor box. Following YOLO, the objectness prediction still predicts the IOU of the ground truth and the proposed box and the class predictions predict the conditional probability of that class given that there is an object.</p><p>当我们移动到锚框时，我们还将类预测机制与空间位置解耦，而是为每个锚盒预测类和对象。 在 YOLO 之后，objectness prediction 仍然预测 ground truth 和提出的 box 的 IOU，并且类预测在给定对象的情况下预测该类的条件概率。</p><p>Using anchor boxes we get a small decrease in accuracy. YOLO only predicts 98 boxes per image but with anchor boxes our model predicts more than a thousand. Without anchor boxes our intermediate model gets 69.5 mAP with a recall of 81%. With anchor boxes our model gets 69.2 mAP with a recall of 88%. Even though the mAP decreases, the increase in recall means that our model has more room to improve.</p><p>使用锚框，我们的准确性会略有下降。 YOLO v1仅预测每张图像 98 个框，但使用锚框，我们的模型预测超过 1000 个框。</p><ul><li>在没有锚框的情况下，我们的中间模型得到 69.5 mAP，召回率为 81%</li><li>使用锚框，我们的模型获得 69.2 mAP，召回率为 88%</li></ul><p>尽管 mAP 降低了，但<strong>召回率的增加意味着我们的模型还有更多的改进空间</strong>。</p><h3 id="dimension-clusters维度聚类-anchor先验框的聚类">2.2.4 Dimension Clusters，维度聚类 —— anchor先验框的聚类</h3><p>We encounter two issues with anchor boxes when using them with YOLO. The first is that the box dimensions are hand picked. The network can learn to adjust the boxes appropriately but if we pick better priors for the network to start with we can make it easier for the network to learn to predict good detections. <strong>Instead of choosing priors by hand, we run k-means clustering on the training set bounding boxes to automatically find good priors.</strong> If we use standard k-means with Euclidean distance larger boxes generate more error than smaller boxes. However, what we really want are priors that lead to good IOU scores, which is independent of the size of the box. Thus for our distance metric we use:</p><p>在将锚框与 YOLO 一起使用时，我们遇到了两个问题。 首先是anchor尺寸是手工挑选的。 网络可以学习适当地调整框，但是如果我们为网络选择更好的先验，我们可以使网络更容易学习预测良好的检测。 我们不是手动选择先验，<strong>而是在训练集边界框上运行 k-means 聚类以自动找到好的先验</strong>。 如果我们使用具有欧几里德距离的标准 k-means，则较大的框会比较小的框产生更多的错误。 然而，我们真正想要的是导致良好 IOU 分数的先验，这与框的大小无关。 因此，对于我们的距离度量，我们使用： <span class="math display">\[ d(\text{box, centroid}) = 1 - \text{IoU(box, centroid)} \]</span> We run k-means for various values of k and plot the average IOU with closest centroid, see Figure 2. We choose k = 5 as a good tradeoff between model complexity and high recall. The cluster centroids are significantly different than hand-picked anchor boxes. There are fewer short, wide boxes and more tall, thin boxes.</p><p>​ 我们对不同的 k 值运行 k-means 并绘制具有最近质心的平均 IOU，见图 2。我们选择 k = 5 作为模型复杂性和高召回率之间的良好折衷。 聚类质心与手工挑选的锚框有很大不同。 短而宽的盒子更少，而高而薄的盒子更多。</p><blockquote><p>矮胖的先验框比较少，高瘦的先验框比较多 —— 这取决于训练集Object的比例</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181137911.png" style="zoom:50%"></p><p>We compare the average IOU to closest prior of our clustering strategy and the hand-picked anchor boxes in Table 1. At only 5 priors the centroids perform similarly to 9 anchor boxes with an average IOU of 61.0 compared to 60.9. If we use 9 centroids we see a much higher average IOU. This indicates that using k-means to generate our bounding box starts the model off with a better representation and makes the task easier to learn.</p><p>我们将平均 IOU 与我们的聚类策略的最接近先验和表 1 中手工挑选的锚框进行比较。在只有 5 个先验时，质心的性能与 9 个锚框相似，平均 IOU 为 61.0，而平均 IOU 为 60.9。 如果我们使用 9 个质心，我们会看到更高的平均 IOU。 这表明使用 k-means 生成我们的边界框以更好的表示启动模型并使任务更容易学习。</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181137172.png" style="zoom:50%"></p><h3 id="direct-location-prediction直接定位预测-预测边界框定位回归">2.2.5 Direct Location Prediction，直接定位预测 —— 预测边界框定位回归</h3><p>When using anchor boxes with YOLO we encounter a second issue: model instability, especially during early iterations. Most of the instability comes from predicting the ( x , y ) (x, y)(x,y) locations for the box. In region proposal networks the network predicts values t x t_xtx and t y t_yty and the ( x , y ) (x, y)(x,y) center coordinates are calculated as:</p><p>在 YOLO 中使用锚框时，我们遇到了第二个问题：<strong>模型不稳定，尤其是在早期迭代期间</strong>。 大多数不稳定性来自于预测边界框的<span class="math inline">\((x, y)\)</span>位置。 在RPN网络中，网络预测值 <span class="math inline">\(t_x\)</span> 和<span class="math inline">\(t_y\)</span> 以及<span class="math inline">\((x, y)\)</span> 中心坐标计算如下： <span class="math display">\[ x=(tx×wa)+wa \]</span></p><p><span class="math display">\[ y=(ty×ha)+ya \]</span></p><blockquote><p>论文中给出的公式其实是错误的，不是<code>-</code>而应该是<code>+</code> 其中：</p><ul><li><span class="math inline">\(t_x, t_y\)</span>为预测框的中心点回归参数</li><li><span class="math inline">\(x, y\)</span>为预测框的中心点坐标</li><li><span class="math inline">\(x_a, y_a, w_a, h_a\)</span>为anchor的中心点坐标和anchor宽度、高度。</li></ul></blockquote><p>For example, a prediction of <span class="math inline">\(t_x = 1\)</span> would shift the box to the right by the width of the anchor box, a prediction of <span class="math inline">\(t_x = −1\)</span> would shift it to the left by the same amount.</p><p>例如，<span class="math inline">\(t_x = 1\)</span>的预测会将框向右移动锚框的宽度，<span class="math inline">\(t_x = -1\)</span> 的预测会将其向左移动相同的量。</p><p>This formulation is unconstrained so any anchor box can end up at any point in the image, regardless of what location predicted the box. With random initialization the model takes a long time to stabilize to predicting sensible offsets. Instead of predicting offsets we follow the approach of YOLO and predict location coordinates relative to the location of the grid cell. This bounds the ground truth to fall between 0 and 1. We use a logistic activation to constrain the network’s predictions to fall in this range.</p><p><strong>由于这个公式是不受约束的，因此任何锚框都可以在图像中的任何点结束</strong>，而不管预测框的位置如何。 通过随机初始化，<strong>模型需要很长时间才能稳定到预测合理的偏移量</strong>。 我们遵循 YOLO 的方法并预测相对于网格单元位置的位置坐标，而不是预测偏移量。 这将基本事实限制在 0 和 1 之间。我们使用逻辑激活来限制网络的预测落在这个范围内。</p><blockquote><p>由于上面那个公式中的预测值<span class="math inline">\(t_x, t_y\)</span>没有限制，所以这种回归方式的预测框<strong>可能出现在图像中的任意一个地方</strong>，举个极端的例子： <img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181145698.png" style="zoom:50%"> 为了改善这种情况，v2中采用了另外一种方法。</p></blockquote><p>​ 假设将priors设置在每一个grid cell的左上角，它的中心点坐标为<span class="math inline">\(c_x, c_y\)</span>，宽度和高度为<span class="math inline">\(p_w, p_h\)</span>，则回归后的anchor坐标为：</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181146285.png" style="zoom:80%"></p><blockquote><p>其中：</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181147044.png"></p></blockquote><blockquote><p>与Faster R-CNN的公式相比，<span class="math inline">\(\sigma(t_x)\)</span>和<span class="math inline">\(\sigma(t_y)\)</span>对预测值进行了限制（将其限制在[0, 1]）—— 回归后anchors的中心坐标不会跑出它的grid cell（因为偏移量是相对anchor的左上角而言的，所以绝对跑不出去！），例子如下： <img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181147711.png" style="zoom:50%"> 这样就实现了 目标中心点落在某个grid cell区域内的目标被其grid cell产生的anchors（priors)预测，中心点没有落在该grid cell中，那么就不由该grid cell去负责预测了。—— 解决了anchor会出现在图像中的任意地方。</p></blockquote><p>Since we constrain the location prediction the parametrization is easier to learn, making the network more stable. Using dimension clusters along with directly predicting the bounding box center location improves YOLO by almost 5% over the version with anchor boxes.</p><p><strong>由于我们限制了位置预测，参数化更容易学习，使网络更稳定</strong>。 使用维度聚类以及直接预测边界框中心位置，与使用锚框的版本相比，YOLO 提高了近 5%。</p><p><strong>好处</strong>：</p><ol type="1"><li>相比使用Faster R-CNN那样的坐标回归，使用①维度聚类得到priors和②限制预测位置的方法，mAP提高了5%。</li></ol><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181148737.png" style="zoom:50%"></p><h3 id="fine-grained-features细粒度特征">2.2.6 Fine-Grained Features，细粒度特征</h3><p>This modified YOLO predicts detections on a 13 × 13 feature map. While this is sufficient for large objects, it may benefit from finer grained features for localizing smaller objects. Faster R-CNN and SSD both run their proposal networks at various feature maps in the network to get a range of resolutions. We take a different approach, simply adding a passthrough layer that brings features from an earlier layer at 26 × 26 resolution.</p><p>这个修改后的 YOLO 预测 13 × 13 特征图上的检测。 虽然这对于大型物体来说已经足够了，但它可能会受益于用于定位较小物体的更细粒度的特征。 Faster R-CNN 和 SSD 都在网络中的各种特征图上运行他们的proposal网络，以获得一系列分辨率。 我们采用不同的方法，只需添加一个直通层，以 26 × 26 的分辨率从较早的层引入特征。</p><blockquote><p>深层的特征图因为感受野大，所以对于检测大目标是比较合适的。<strong>但对于小目标来说，太大的感受野并不是一件好事。</strong>为了解决这个问题，作者加入了一个直通层（passthrough layer）来融合相对没那么深的特征图，以此来加强小目标的检测效果。 13×13的特征图融合26×26的特征图</p></blockquote><p>The passthrough layer concatenates the higher resolution features with the low resolution features by stacking adjacent features into different channels instead of spatial locations, similar to the identity mappings in ResNet. This turns the 26 × 26 × 512 feature map into a 13 × 13 × 2048 feature map, which can be concatenated with the original features. Our detector runs on top of this expanded feature map so that it has access to fine grained features. This gives a modest 1% performance increase.</p><p>直通层通过将相邻特征堆叠到不同的通道而不是空间位置来连接更高分辨率的特征和低分辨率的特征，类似于 ResNet 中的恒等映射。 这将 26 × 26 × 512 的特征图变成了 13 × 13 × 2048 的特征图，可以与原始特征进行拼接。 我们的检测器在这个扩展的特征图之上运行，因此它可以访问细粒度的特征。 这会带来 1% 的适度性能提升。</p><blockquote><p>这里的2048是没有使用1×1卷积降维的情况</p><p>但在源码中，作者使用了1×1卷积进行了降维</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181231042.png"></p><h3 id="multi-scale-training多尺度训练">2.2.7 Multi-scale Training，多尺度训练</h3><p>The original YOLO uses an input resolution of 448 × 448. With the addition of anchor boxes we changed the resolution to 416×416. However, since our model only uses convolutional and pooling layers it can be resized on the fly. We want YOLOv2 to be robust to running on images of different sizes so we train this into the model.</p><p>原始 YOLO 使用 448 × 448 的输入分辨率。通过添加锚框，我们将分辨率更改为 416×416。 然而，由于我们的模型只使用卷积层和池化层，它可以动态调整大小。 我们希望 YOLOv2 能够在不同大小的图像上运行，因此我们将其训练到模型中。</p><blockquote><p>416×416是因为想要得到奇数个grid cell，这样它们的中心点就是一个像素而不是比0.5个像素，经过下采样后，最终的预测特征图大小为13×13，它是奇数的，所以有很明显的中心点。</p></blockquote><p>Instead of fixing the input image size we change the network every few iterations. Every 10 batches our network randomly chooses a new image dimension size. Since our model downsamples by a factor of 32, we pull from the following multiples of 32: {320, 352, …, 608}. Thus the smallest option is 320 × 320 and the largest is 608 × 608. We resize the network to that dimension and continue training.</p><p>我们不是固定输入图像的大小，而是<strong>每隔几次迭代就改变网络</strong>。 每 10 批我们的网络随机选择一个新的图像尺寸。 由于我们的模型下采样了 32 倍（缩放因子32；416-&gt;13），我们从以下 32 的倍数中提取：{320, 352, …, 608}。 因此，最小的选项是 320 × 320，最大的选项是 608 × 608。我们将网络调整到该维度并继续训练。</p><blockquote><p>这里所说的batches指的是Epoch {320, 352, …, 608} = {32×10, 32×11, …, 32×19}</p></blockquote><p>This regime forces the network to learn to predict well across a variety of input dimensions. This means the same network can predict detections at different resolutions. The network runs faster at smaller sizes so YOLOv2 offers an easy tradeoff between speed and accuracy.</p><p>这种制度<strong>迫使网络学会在各种输入维度上进行良好的预测</strong>。 这意味着同一个网络可以预测不同分辨率的检测。 网络在较小的尺寸下运行得更快，因此 YOLOv2 在速度和准确性之间提供了一个简单的权衡。</p><p>At low resolutions YOLOv2 operates as a cheap, fairly accurate detector. At 288 × 288 it runs at more than 90 FPS with mAP almost as good as Fast R-CNN. This makes it ideal for smaller GPUs, high framerate video, or multiple video streams.</p><p>在低分辨率下，YOLOv2 是一种廉价且相当准确的检测器。 在 288 × 288 时，它以超过 90 FPS 的速度运行，mAP 几乎与 Fast R-CNN 一样好。 这使其非常适合较小的 GPU、高帧率视频或多个视频流。</p><p>At high resolution YOLOv2 is a state-of-the-art detector with 78.6 mAP on VOC 2007 while still operating above real-time speeds. See Table 3 for a comparison of YOLOv2 with other frameworks on VOC 2007 (Figure 4).</p><p>在高分辨率下，YOLOv2 是最先进的检测器，在 VOC 2007 上具有 78.6 mAP，同时仍以高于实时速度运行。 有关 YOLOv2 与其他框架在 VOC 2007 上的比较，请参见表 3（图 4）。</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181251406.png"></p><blockquote><p>Note:</p><ul><li>这里采用多尺度训练并不是训练backbone，而是训练网络定位和分类回归！</li><li>训练backbone用的448×448的输入图片！</li></ul></blockquote><h2 id="yolo-v2的backbone-darknet-19">2.3 YOLO v2的backbone —— Darknet-19</h2><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181251669.png" style="zoom:67%"> 这里举例的Darknet-19输入为224×224。</p><ul><li>19代表卷积层的个数</li></ul><p>Darknet-19（224×224）only requires 5.58 billion operations to process an image yet achieves 72.9% top-1 accuracy and 91.2% top-5 accuracy on ImageNet.</p><p>Darknet-19 （224×224）只需要 <strong>55.8 亿次操作</strong>来处理图像，但在 ImageNet 上实现了 <strong>72.9% 的 top-1 准确率和 91.2% 的 top-5 准确率</strong>。</p><blockquote><p>虽然YOLO v2的backbone在训练时使用的输入图片大小为448×448，但这里为了展示Darknet-19的性能，将其输入设置为了224×224，因为其他的分类网络一般的输入均为224×224。</p></blockquote><h2 id="yolo-v2的整体框架">2.4 YOLO v2的整体框架</h2><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181252587.png" style="zoom:80%"> 其中：</p><ul><li><code>Filters</code>代表卷积核的个数（也就是输出的通道数）</li><li><code>Size</code>为卷积核大小<ul><li>默认stride=1</li><li>加了<code>/</code>，后面的数表示stride</li></ul></li><li>每一个<code>Convolutional</code>都包含三个部分<ul><li>Conv2d（不包含bias —— 因为有了BN）</li><li>BN</li><li>LeakyReLU（也就是PReLU）</li></ul></li><li>最后的<code>Conv2d 125 1×1</code>就只是一个1×1卷积，没有BN和激活函数</li></ul><p>作者将自己提出的分类网络Darknet-19用到了YOLO v2这个目标检测网络中并修改了其部分结构：</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181253939.png" style="zoom:67%"></p><ol type="1"><li>移除了最后一层卷积和其之后的层结构</li><li>在后面添加三个3×3的卷积层（每个卷积层的卷积核个数为1024）</li><li>在添加的3个卷积层中，第二个卷积层之后增加PassThrough层（直通层）</li><li>最后接上一个1×1端卷积层，其输出个数为所需检测的目标参数（这里为(5+20)×5）</li></ol><p><strong>所需检测的目标参数</strong>：</p><p>对于VOC数据集，网络会预测5个Anchor，每个Anchor有5个参数(x, y, w, h, confidence)，其中(x, y, w, h)为anchor的回归信息；confidence为该anchor与GTBox的IoU值。VOC数据集有20个类别，所以对于每一个Anchor均需预测20个类别分数。 故需要(5 + 20) * 5 = 125个需要预测的参数</p><blockquote><p>与YOLO v1不同，YOLO v1是grid cell预测类别分数，而v2是每一个anchor均需要预测20个分数。</p></blockquote><h2 id="yolo-v2训练细节">2.5 YOLO v2训练细节</h2><h3 id="论文中没有提及">2.5.1 论文中没有提及</h3><ol type="1"><li>如何匹配正负样本</li><li>如何计算误差</li></ol><p>这两部分会在YOLO v3中进行细讲😂</p><h3 id="论文提及的训练细节">2.5.2 论文提及的训练细节</h3><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211181259003.png"></p><ol type="1"><li>在优化器中使用了权重衰减和动量</li><li>使用了与YOLO v1相同的数据增强处理以及SSD中采用的随机裁剪和颜色偏移的增强方式</li><li>在COCO和VOC上的训练策略是相同</li></ol><h1 id="yolo-v3">3. YOLO v3</h1><p>论文题目：<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=YOLOv3&amp;spm=1001.2101.3001.7020">YOLOv3</a>: An Incremental Improvement 论文地址：https://arxiv.org/abs/1804.02767</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211190959381.png"> 2018年发表在CVPR，其论文中的内容比较少 —— 没有太多的创新点，基本上就是整合了当时比较主流网络的优点。</p><h2 id="yolo-v3与其他网络的对比">3.1 YOLO v3与其他网络的对比</h2><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211191000702.png" style="zoom:67%"> 数据集为MS COCO数据，相比于其他网络而言，YOLO v3的速度是非常快的，但其AP并不是很高。</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211191000454.png" style="zoom:50%"> 该图的纵坐标为COCO APIoU=0.5 —— PASCAL VOC的mAP。通过该指标来看，YOLO v3的速度和准确率都比较高。</p><h2 id="yolo-v3网络架构">3.2 YOLO v3网络架构</h2><h3 id="backbone-darknet-53">3.2.1 backbone —— Darknet-53</h3><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211191001938.png" style="zoom:80%"></p><p>修改了backbone —— Darknet-19 -&gt; Darknet-53</p><p>根据右上角的表格我们可以知道，Darknet-19变为Darknet-53提升了分类的准确率（和ResNet-152持平），<strong>但其FPS腰斩</strong>。</p><blockquote><p>虽然速度降低了，但重点不是在这里。而是<strong>Darknet-53准确率和ResNet-152持平，但速度是其2倍</strong>！ —— Darknet-53也是非常强劲的分类网络</p></blockquote><p>53表示卷积层有53个，注意最后分类层中全连接层被卷积层替代，所以有53个卷积层。</p><p><strong>Q</strong>：为什么Darknet-53的效果比ResNet-152的效果要好？ <strong>A</strong>：Darknet-53没有MaxPooling层（<strong>所有下采样都是通过卷积层实现的</strong>）</p><p><strong>Q</strong>：为什么Darknet-53的速度比ResNet-152要快？ <strong>A</strong>：ResNet-152的卷积核个数比Darknet-53要多很多 —— Darknet-53的参数要少，运算量少 -&gt; 速度快</p><h3 id="整体框架">3.2.2 整体框架</h3><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211191003619.png"> 从图中可以看到，YOLO v3的预测特征图有3个：</p><ul><li>Predict one: 13×13 -&gt; 预测大目标</li><li>Predict one: 26×26 -&gt; 预测中目标</li><li>Predict one: 52×52 -&gt; 预测小目标</li></ul><h2 id="目标检测框的预测机制">3.3 目标检测框的预测机制</h2><p>v3采用了和v2一样的机制。</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211191014118.png" style="zoom:67%"> 其中：</p><ul><li><p><span class="math inline">\(t_x, t_y, t_w, t_h\)</span>为预测的关于anchor的偏移量</p></li><li><p><span class="math inline">\(\sigma(\cdot)\)</span>为Sigmoid函数 —— 将输入映射到 <span class="math inline">\([ 0 , 1 ]\)</span></p></li><li><p><span class="math inline">\(p_w, p_h\)</span>为先验框的宽度和高度</p></li><li><p><span class="math inline">\(b_x, b_y, b_w, b_h\)</span> 为最终边界框的中心点坐标以及宽度、高度</p></li><li><p><span class="math inline">\(c_x, c_y\)</span>当前grid cell左上角点的坐标</p></li><li><p>虚线的矩形框对应的是priors（anchor） —— 这里我们只需关心其宽度和高度这个参数即可</p></li><li><p>图中蓝色矩形框是网络预测的最终目标位置以及大小</p></li></ul><blockquote><p>Sigmoid函数的存在使得预测值不会超过当前的grid cell —— 和v2是一样的</p></blockquote><p>通过图中的公式就可以将网络预测的回归参数转化为最终的目标边界框具体坐标。</p><h2 id="grid-cell的priors的比例">3.4 grid cell的priors的比例</h2><p>针对每一个预测特征图而言，每一个先验框均有3种比例（3种不同的anchor模板）。 <img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211191018916.png"></p><p>因为YOLO v3有3种不同尺寸的预测特征图，所以将不同尺寸的anchor分配给不同的预测特征图，具体细节如下表所示。</p><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211191018349.png"></p><blockquote><p>感受野越大，预测的目标尺寸应该越大 反之亦然</p></blockquote><h2 id="正负样本的匹配">3.5 正负样本的匹配</h2><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211191018709.png" style="zoom:67%"> YOLOv3 使用<strong>逻辑回归</strong>预测每个边界框的对象度得分。</p><ul><li>如果边界框先验与GTBox的重叠比任何其他边界框先验多，则该值应为 1。</li><li>如果边界框先验不是最好的，但确实与GTBox重叠超过某个阈值，我们将忽略预测，遵循 [17]。 我们使用 0.5 的阈值。 与 [17] 不同，我们的系统只为每个GTBox分配一个边界框先验。 如果<strong>未</strong>将边界框先验分配给GTBox，则不会对坐标或类别预测造成损失，只会对对象性造成损失。</li></ul><blockquote><p>[17]为Faster R-CNN</p></blockquote><p>针对每一个GT而言都会分配一个正样本 —— 一张图片中有几个GT目标就有几个正样本。</p><p><strong>分配原则</strong>：</p><ul><li>先验框与GTBox的IoU最大的作为正样本</li><li>如果其他先验框与该GTBox的IoU的值不是最大，但IoU超过了设置的阈值（0.5） -&gt; 直接丢弃这个先验框，即这个先验框既不是正样本也不是负样本，而是直接弃用</li><li>剩下与GTBox的IoU不是最大且小于设定阈值的先验框均设置为负样本</li><li>如果先验框没有分配给某一个GTBox（即当前的先验框不是正样本的话），那么它既没有定位损失，也没有类别损失，仅仅只有confidence损失（与GTBox的IoU损失）</li></ul><h2 id="yolo-v3源码的正负样本匹配策略">3.6 YOLO v3源码的正负样本匹配策略</h2><p>如果我们按上论文中（也就是上面的这种）正负样本匹配策略的话，会发现<strong>正样本的数量太少了</strong> —— 导致网络很难训练。</p><p>针对每一个预测特征图而言，每一个先验框均有3种比例（3种不同的anchor模板），如下所示： <img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211191021465.png"></p><hr><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211191021846.png"></p><ol type="1"><li>将每一个Anchor模板与GTBox进行IoU的计算（将二者的左上角重合后再计算IoU）</li><li>设置IoU阈值（0.3），<strong>只要Anchor与GTBox的IoU &gt; 0.3，该Anchor都会被设置为正样本</strong> —— 只有第二个anchor模板符合条件，为正样本</li><li>将GTBox映射到预测特征图上（其中<strong>黑色的圆点</strong>为GTBox的中心点，<strong>×</strong>为GTBox所属grid cell的左上角坐标）</li><li>GTBox中心点落在哪个grid cell，哪个grid cell对应的anchor模板2就为正样本</li></ol><hr><p><strong>Q</strong>：如果这三个anchor模板与GTBox的IoU均大于阈值呢？ <strong>A</strong>：如果出现这种情况，那么GTBox所属的grid cell的3个anchor模板<strong>均为正样本</strong></p><p>这种策略中，只要Anchor与GTBox的IoU大于阈值，那么它就是正样本，小于阈值就是负样本（没有论文策略中废弃Anchor的做法了） —— 目的就是为了<strong>扩充正样本的数量</strong>。</p><blockquote><p>这种策略在实践中的效果确实比论文中的策略要好一些</p></blockquote><h2 id="损失函数-1">3.7 损失函数</h2><p>YOLO v3整体损失计算如下： <span class="math display">\[ L(o, c, O, C, l, g) = \lambda_1 \underset{置信度损失}{L_{\text{conf}}(o, c)} + \lambda_2 \underset{分类损失}{L_{\text{cls}}(O, C)} + \lambda_3 \underset{定位损失}{L_{\text{loc}}(l, g)} \]</span></p><blockquote><p>其中<span class="math inline">\(\lambda_1, \lambda_2, \lambda_3\)</span>为平衡系数。</p></blockquote><h3 id="置信度损失confidence-loss">3.7.1 置信度损失（Confidence Loss）</h3><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211191025683.png"></p><p>论文中说使用的是逻辑回归，而逻辑回归一般都是BCE损失（即二值交叉熵损失）</p><p><strong>Q</strong>：预测目标边界框是Anchor priors吗？ <strong>A</strong>：并不是，看下图。 <img src="https://img-blog.csdnimg.cn/6c03011d50a940d1994c2f64deaa37d8.png#pic_center" alt="在这里插入图片描述"> 其中</p><ul><li>蓝色框为Anchor priors</li><li>绿色框为GTBox</li><li>黄色框为预测目标边界框</li></ul><p>预测目标边界框是Anchor根据预测值回归得到的，即o i o_ioi是黄色框与绿色框的IoU值。</p><h3 id="分类类别损失classes-loss">3.7.2 分类/类别损失（Classes Loss）</h3><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211191026829.png"></p><p>这里的分类损失也是BCE损失。</p><hr><p><strong>例子</strong>：</p><p><img src="https://img-blog.csdnimg.cn/4eab17e4943841838cbbef1163f3dbf5.png" alt="在这里插入图片描述"> 我们需要检测图片中的[老虎，豹子，猫]这三个类别；左下角的图片中有两个类别，且均为猫。</p><ul><li>目标1的GT概率为[0, 0, 1] —— one-hot编码</li><li>目标2的GT概率为[0, 0, 1] —— one-hot编码</li></ul><p>假设网络的预测概率为：经过Sigmoid函数处理后得到的</p><ul><li>目标1的预测概率为[0.1, 0.8, 0.9]</li><li>目标2的预测概率为[0.2, 0.7, 0.8]</li></ul><p>我们发现，预测每个目标预测值的和并不是等于1的，之前我们使用Softmax交叉熵的时候，预测概率之和为1；而这里用的是BCE交叉熵，每一个输出值是直接通过Sigmoid函数进行处理的，<strong>每个预测值之间是互不干扰、相互独立的</strong>，所以就可能会出现 —— 该目标既有可能是豹子也有可能是猫的情况（就是可能出现多个概率较大的类别）。</p><hr><p><strong>验证</strong>：验证类别损失的BCE是否正确</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token triple-quoted-string string">"""
    reduction="none"表示求出BCELoss后不进行任何处理
    如果不设置，默认是对其进行求均值处理 -> 返回一个均值
"""</span>
loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">"none"</span><span class="token punctuation">)</span>
<span class="token comment"># loss = torch.nn.BCELoss()  # 0.5784000158309937</span>
predicted_cls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.8</span><span class="token punctuation">,</span> <span class="token number">0.9</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.7</span><span class="token punctuation">,</span> <span class="token number">0.8</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token comment"># 上方Sigmod处理后的两个框对应类别预测值</span>
predict_cls_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>predicted_cls<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

gt_cls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
gt_cls_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>gt_cls<span class="token punctuation">)</span>

loss_res <span class="token operator">=</span> loss<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>predict_cls_tensor<span class="token punctuation">,</span> target<span class="token operator">=</span>gt_cls_tensor<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"PyTorch官方实现的结果: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>np<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>loss_res<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> decimals<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>  <span class="token comment"># decimals: 将数组四舍五入到给定的小数位数。</span>


<span class="token keyword">def</span> <span class="token function">bce</span><span class="token punctuation">(</span>c<span class="token punctuation">,</span> o<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 这里的np.log就是ln（以e为底）</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token punctuation">(</span>o <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span> o<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span> c<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> decimals<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>


pn <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>predicted_cls<span class="token punctuation">)</span>
gn <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>gt_cls<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"自定义实现的结果: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>bce<span class="token punctuation">(</span>pn<span class="token punctuation">,</span> gt_cls<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>


<span class="token triple-quoted-string string">"""
    PyTorch官方实现的结果: [[0.10536 1.60944 0.10536]
                         [0.22314 1.20397 0.22314]]
    自定义实现的结果: [[0.10536 1.60944 0.10536]
                    [0.22314 1.20397 0.22314]]
"""</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="定位损失location-loss">3.7.3 定位损失（Location Loss）</h3><p><img src="https://cdn.jsdelivr.net/gh/Unicorn-acc/blogimgs/imgs01/202211191038389.png"></p><p>在训练阶段，定位损失使用的是<strong>平方误差和损失</strong>（也就是我们常说的<span class="math inline">\(L_2\)</span>损失）。</p><blockquote><p>这里的定位损失函数做一个简单了解就可以了，因为后面的v3-SPP、v4、v5并不是使用平方误差和来计算定位损失，而是使用的CIoU Loss。</p></blockquote><h1 id="yolo-v3-spp">4. YOLO v3-SPP</h1><h2 id="v3-spp相比v3的改进之处">4.1 v3-SPP相比v3的改进之处</h2><p><strong>性能对比</strong>：</p><table><thead><tr class="header"><th>Model</th><th>输入图片尺寸</th><th>COCO mAP@0.5:0.95</th><th>COCO mAP@0.5</th></tr></thead><tbody><tr class="odd"><td>YOLO v3</td><td>512×512</td><td>32.7</td><td>57.7</td></tr><tr class="even"><td>YOLO v3-SPP</td><td>512×512</td><td>35.6 (+2.90)</td><td>59.5 (+1.80)</td></tr><tr class="odd"><td>YOLO v3-SPP-ultralytics</td><td>512×512</td><td><strong>42.6 (+9.90)</strong></td><td><strong>62.4 (+4.70)</strong></td></tr></tbody></table><blockquote><p>ultralytics的YOLO v3-SPP之所以性能提升更多是因为使用了更多的trick</p></blockquote><p><strong>tricks</strong>：</p><ol type="1"><li>Mosaic图像增强</li><li>SPP模块（Spatial Pyramid Pooling，空间金字塔池化）</li><li>CIoU Loss</li><li>Focal Loss（焦点损失）</li></ol><blockquote><p>前三个是ultralytics使用的，Focal Loss虽然在网络有已经实现了，但默认不使用（效果并不是很好）</p></blockquote></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://Unicorn-acc.github.io">Miraclo</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://unicorn-acc.github.io/posts/62071.html">http://unicorn-acc.github.io/posts/62071.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://Unicorn-acc.github.io" target="_blank">Miraclo’s Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/DeepLearning/">DeepLearning</a></div><div class="post_share"><div class="social-share" data-image="https://w.wallhaven.cc/full/1p/wallhaven-1pjml1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/25470.html"><img class="prev-cover" src="https://w.wallhaven.cc/full/zy/wallhaven-zyxvqy.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">JVM系列-06-本地方法接口</div></div></a></div><div class="next-post pull-right"><a href="/posts/60257.html"><img class="next-cover" src="https://w.wallhaven.cc/full/vq/wallhaven-vqmyq3.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">目标检测-03-RetinaNet</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/5074.html" title="图像分类_CIFAR10数据集"><img class="cover" src="https://w.wallhaven.cc/full/kx/wallhaven-kx3p1q.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-07</div><div class="title">图像分类_CIFAR10数据集</div></div></a></div><div><a href="/posts/47625.html" title="图像分类_Fashion-MNIST数据集(ResNet18进行迁移学习)"><img class="cover" src="https://w.wallhaven.cc/full/1p/wallhaven-1pjml1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-04</div><div class="title">图像分类_Fashion-MNIST数据集(ResNet18进行迁移学习)</div></div></a></div><div><a href="/posts/56670.html" title="目标检测&#x2F;分割_PASCAL VOC2012数据集与制作自己的数据集(VOC格式)"><img class="cover" src="https://w.wallhaven.cc/full/1p/wallhaven-1pjml1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-11</div><div class="title">目标检测&#x2F;分割_PASCAL VOC2012数据集与制作自己的数据集(VOC格式)</div></div></a></div><div><a href="/posts/2490.html" title="图像分类_花分类数据集"><img class="cover" src="https://w.wallhaven.cc/full/x6/wallhaven-x619o3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-06</div><div class="title">图像分类_花分类数据集</div></div></a></div><div><a href="/posts/64179.html" title="图像分类_00_LeNet_Pytorch官方示例"><img class="cover" src="https://w.wallhaven.cc/full/x6/wallhaven-x619o3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-07</div><div class="title">图像分类_00_LeNet_Pytorch官方示例</div></div></a></div><div><a href="/posts/5932.html" title="目标检测-03-FPN特征金字塔网络(Feature Pyramid Network)"><img class="cover" src="https://w.wallhaven.cc/full/kx/wallhaven-kx3p1q.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-17</div><div class="title">目标检测-03-FPN特征金字塔网络(Feature Pyramid Network)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-text">0. 引言</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E6%8E%92"><span class="toc-text">0.1 安排</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E7%9A%84%E6%AD%A5%E9%AA%A4"><span class="toc-text">0.2 学习代码的步骤</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#yolo-v1"><span class="toc-text">1. YOLO v1</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-text">1.1 核心思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%E5%88%92%E5%88%86grid-cell"><span class="toc-text">1.2 第一步：划分grid cell</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%E6%AF%8F%E4%B8%AAgrid-cell%E9%9C%80%E8%A6%81%E5%81%9A%E7%9A%84%E4%BA%8B%E6%83%85"><span class="toc-text">1.3 第二步：每个grid cell需要做的事情</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-text">1.4 网络结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">1.5 损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#bbox%E6%8D%9F%E5%A4%B1"><span class="toc-text">1.5.1 BBox损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#confidence%E6%8D%9F%E5%A4%B1"><span class="toc-text">1.5.2 confidence损失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B1%BB%E5%88%AB%E6%8D%9F%E5%A4%B1"><span class="toc-text">1.5.3 类别损失</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#yolo-v1%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">1.6 YOLO v1存在的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#yolo-v1%E4%B8%AD%E7%9A%84bounding-box-%E5%92%8C-anchor-%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB"><span class="toc-text">1.7 YOLO v1中的Bounding Box 和 Anchor 有什么区别？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#yolo-v2"><span class="toc-text">2. YOLO v2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#yolo-v2-%E4%B8%8E%E5%85%B6%E4%BB%96%E7%BD%91%E7%BB%9C%E7%9A%84%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"><span class="toc-text">2.1 YOLO v2 与其他网络的性能对比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#yolo-v2%E7%9B%B8%E6%AF%94v1%E7%9A%84%E5%B0%9D%E8%AF%95"><span class="toc-text">2.2 YOLO v2相比v1的尝试</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#batch-normalization"><span class="toc-text">2.2.1 Batch Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#high-resolution-classifier%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-text">2.2.2 High Resolution Classifier，高分辨率分类器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#convolutional-with-anchor-boxes%E5%9F%BA%E4%BA%8Eanchor-boxes%E7%9A%84%E5%8D%B7%E7%A7%AF"><span class="toc-text">2.2.3 Convolutional With Anchor Boxes，基于Anchor Boxes的卷积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dimension-clusters%E7%BB%B4%E5%BA%A6%E8%81%9A%E7%B1%BB-anchor%E5%85%88%E9%AA%8C%E6%A1%86%E7%9A%84%E8%81%9A%E7%B1%BB"><span class="toc-text">2.2.4 Dimension Clusters，维度聚类 —— anchor先验框的聚类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#direct-location-prediction%E7%9B%B4%E6%8E%A5%E5%AE%9A%E4%BD%8D%E9%A2%84%E6%B5%8B-%E9%A2%84%E6%B5%8B%E8%BE%B9%E7%95%8C%E6%A1%86%E5%AE%9A%E4%BD%8D%E5%9B%9E%E5%BD%92"><span class="toc-text">2.2.5 Direct Location Prediction，直接定位预测 —— 预测边界框定位回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fine-grained-features%E7%BB%86%E7%B2%92%E5%BA%A6%E7%89%B9%E5%BE%81"><span class="toc-text">2.2.6 Fine-Grained Features，细粒度特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-scale-training%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AE%AD%E7%BB%83"><span class="toc-text">2.2.7 Multi-scale Training，多尺度训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#yolo-v2%E7%9A%84backbone-darknet-19"><span class="toc-text">2.3 YOLO v2的backbone —— Darknet-19</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#yolo-v2%E7%9A%84%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6"><span class="toc-text">2.4 YOLO v2的整体框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#yolo-v2%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82"><span class="toc-text">2.5 YOLO v2训练细节</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E4%B8%AD%E6%B2%A1%E6%9C%89%E6%8F%90%E5%8F%8A"><span class="toc-text">2.5.1 论文中没有提及</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E6%8F%90%E5%8F%8A%E7%9A%84%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82"><span class="toc-text">2.5.2 论文提及的训练细节</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#yolo-v3"><span class="toc-text">3. YOLO v3</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#yolo-v3%E4%B8%8E%E5%85%B6%E4%BB%96%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-text">3.1 YOLO v3与其他网络的对比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#yolo-v3%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-text">3.2 YOLO v3网络架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#backbone-darknet-53"><span class="toc-text">3.2.1 backbone —— Darknet-53</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6"><span class="toc-text">3.2.2 整体框架</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A1%86%E7%9A%84%E9%A2%84%E6%B5%8B%E6%9C%BA%E5%88%B6"><span class="toc-text">3.3 目标检测框的预测机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#grid-cell%E7%9A%84priors%E7%9A%84%E6%AF%94%E4%BE%8B"><span class="toc-text">3.4 grid cell的priors的比例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E7%9A%84%E5%8C%B9%E9%85%8D"><span class="toc-text">3.5 正负样本的匹配</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#yolo-v3%E6%BA%90%E7%A0%81%E7%9A%84%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E5%8C%B9%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-text">3.6 YOLO v3源码的正负样本匹配策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-1"><span class="toc-text">3.7 损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%AE%E4%BF%A1%E5%BA%A6%E6%8D%9F%E5%A4%B1confidence-loss"><span class="toc-text">3.7.1 置信度损失（Confidence Loss）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E7%B1%BB%E5%88%AB%E6%8D%9F%E5%A4%B1classes-loss"><span class="toc-text">3.7.2 分类&#x2F;类别损失（Classes Loss）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%BD%8D%E6%8D%9F%E5%A4%B1location-loss"><span class="toc-text">3.7.3 定位损失（Location Loss）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#yolo-v3-spp"><span class="toc-text">4. YOLO v3-SPP</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#v3-spp%E7%9B%B8%E6%AF%94v3%E7%9A%84%E6%94%B9%E8%BF%9B%E4%B9%8B%E5%A4%84"><span class="toc-text">4.1 v3-SPP相比v3的改进之处</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Miraclo</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">人只有在走上坡路的时候才会累和迷茫。</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div><hr><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.1},options:{renderActions:{findScript:[10,t=>{for(const n of document.querySelectorAll('script[type^="math/tex"]')){var e=!!n.type.match(/; *mode=display/),e=new t.options.MathItem(n.textContent,t.inputJax[0],e),a=document.createTextNode("");n.parentNode.replaceChild(a,n),e.start={node:a,delim:"",n:0},e.end={node:a,delim:"",n:0},t.math.push(e)}},""],insertScript:[200,()=>{document.querySelectorAll("mjx-container").forEach(t=>{t.hasAttribute("display")?btf.wrap(t,"div",{class:"mathjax-overflow"}):btf.wrap(t,"span",{class:"mathjax-overflow"})})},"",!1]}}};const a=document.createElement("script");a.src="https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js",a.id="MathJax-script",a.async=!0,document.head.appendChild(a)}</script></div><link rel="stylesheet" href="/css/Lete.css"><script src="/js/custom.js"></script><script src="/js/mouth.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script data-pjax>var parent,child;document.getElementById("recent-posts")&&"/"===location.pathname&&(parent=document.getElementById("recent-posts"),child='<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://Unicorn-acc.github.io/categories/Java技术栈/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 Java技术栈相关 (54)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://Unicorn-acc.github.io/categories/深度学习笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 深度学习笔记相关 (20)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://Unicorn-acc.github.io/categories/深度学习笔记/网络模型/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📒 网络模型 (15)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://Unicorn-acc.github.io/categories/数据库/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">‍👓 数据库相关 (36)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item" style="visibility: hidden"></div><div class="magnet_item" style="visibility: hidden"></div><a class="magnet_link_more"  href="http://Unicorn-acc.github.io/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>',console.log("已挂载magnet"),parent.insertAdjacentHTML("afterbegin",child))</script><style>#catalog_magnet{flex-wrap:wrap;display:flex;width:100%;justify-content:space-between;padding:10px 10px 0 10px;align-content:flex-start}.magnet_item{flex-basis:calc(33.333333333333336% - 5px);background:#f2f2f2;margin-bottom:10px;border-radius:8px;transition:all .2s ease-in-out}.magnet_item:hover{background:#b30070}.magnet_link_more{color:#555}.magnet_link{color:#000}.magnet_link:hover{color:#fff}@media screen and (max-width:600px){.magnet_item{flex-basis:100%}}.magnet_link_context{display:flex;padding:10px;font-size:16px;transition:all .2s ease-in-out}.magnet_link_context:hover{padding:10px 20px}</style><style></style></body></html>